{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Directory Structure\n",
    "\n",
    "```plaintext\n",
    ".\n",
    "├── pretrain/\n",
    "│   ├── img/\n",
    "│   │   ├── captcha1.jpg\n",
    "│   │   ├── captcha2.png\n",
    "│   │   └── ...\n",
    "│   ├── model/\n",
    "│   │   ├── crnn_model-resnet18-gru_pretrain.pt\n",
    "│   └── output/\n",
    "│       ├── train_val_results_pretrain.csv\n",
    "│       └── ...\n",
    "├── new/\n",
    "│   ├── img-1/\n",
    "│   │   ├── img/\n",
    "│   │   │   ├── captcha1.jpg\n",
    "│   │   │   ├── captcha2.png\n",
    "│   │   │   └── ...\n",
    "│   │   ├── model/\n",
    "│   │   │   ├── crnn_model-resnet18-gru_img-1.pt\n",
    "│   │   └── output/\n",
    "│   │       ├── train_val_results_img-1.csv\n",
    "│   │       └── ...\n",
    "│   ├── img-2/\n",
    "│   │   ├── img/\n",
    "│   │   │   ├── captcha3.jpg\n",
    "│   │   │   ├── captcha4.png\n",
    "│   │   │   └── ...\n",
    "│   │   ├── model/\n",
    "│   │   │   ├── crnn_model-resnet18-gru_img-2.pt\n",
    "│   │   └── output/\n",
    "│   │       ├── train_val_results_img-2.csv\n",
    "│   │       └── ...\n",
    "│   ├── classifier/\n",
    "│   │   ├── model/\n",
    "│   │   │   ├── classifier_model.pt\n",
    "│   │   └── output/\n",
    "│   │       ├── [Classification Model Output Files]\n",
    "│   └── ... [Other img-N Folders]\n",
    "├── train-evaluate/\n",
    "│   ├── img/\n",
    "│   │   ├── captcha5.jpg\n",
    "│   │   ├── captcha6.png\n",
    "│   │   └── ...\n",
    "│   ├── model/\n",
    "│   │   ├── crnn_model-resnet18-gru_train-evaluate.pt\n",
    "│   └── output/\n",
    "│       ├── train_val_results_train-evaluate.csv\n",
    "│       └── ...\n",
    "├── predict/\n",
    "│   ├── new_captcha1.jpg\n",
    "│   ├── new_captcha2.png\n",
    "│   └── predictions.csv\n",
    "└── main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已創建或已存在資料夾: ./pretrain\\img\n",
      "已創建或已存在資料夾: ./pretrain\\model\n",
      "已創建或已存在資料夾: ./pretrain\\output\n",
      "已創建或已存在資料夾: ./new\\img-1\\img\n",
      "已創建或已存在資料夾: ./new\\img-1\\model\n",
      "已創建或已存在資料夾: ./new\\img-1\\output\n",
      "已創建或已存在資料夾: ./new\\img-2\\img\n",
      "已創建或已存在資料夾: ./new\\img-2\\model\n",
      "已創建或已存在資料夾: ./new\\img-2\\output\n",
      "已創建或已存在資料夾: ./new\\img-3\\img\n",
      "已創建或已存在資料夾: ./new\\img-3\\model\n",
      "已創建或已存在資料夾: ./new\\img-3\\output\n",
      "已創建或已存在資料夾: ./new\\img-4\\img\n",
      "已創建或已存在資料夾: ./new\\img-4\\model\n",
      "已創建或已存在資料夾: ./new\\img-4\\output\n",
      "已創建或已存在資料夾: ./new\\img-5\\img\n",
      "已創建或已存在資料夾: ./new\\img-5\\model\n",
      "已創建或已存在資料夾: ./new\\img-5\\output\n",
      "已創建或已存在資料夾: ./train-evaluate\\img\n",
      "已創建或已存在資料夾: ./train-evaluate\\model\n",
      "已創建或已存在資料夾: ./train-evaluate\\output\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def create_folder_structure(base_dir='./', num_new_imgs=5):\n",
    "    \"\"\"\n",
    "    建立預定的資料夾結構，包括 pretrain、new/img-1 至 new/img-5、以及 train-evaluate 資料夾。\n",
    "    \n",
    "    :param base_dir: 基礎目錄，預設為當前目錄。\n",
    "    :param num_new_imgs: 在 'new/' 資料夾下要建立多少個 'img-數字' 子資料夾。\n",
    "    \"\"\"\n",
    "    # 定義 pretrain 資料夾結構\n",
    "    pretrain_structure = [\n",
    "        os.path.join(base_dir, \"pretrain\", \"img\"),\n",
    "        os.path.join(base_dir, \"pretrain\", \"model\"),\n",
    "        os.path.join(base_dir, \"pretrain\", \"output\"),\n",
    "    ]\n",
    "    \n",
    "    # 定義 new 資料夾結構\n",
    "    new_base = os.path.join(base_dir, \"new\")\n",
    "    new_structures = []\n",
    "    for i in range(1, num_new_imgs + 1):\n",
    "        img_folder = os.path.join(new_base, f\"img-{i}\", \"img\")\n",
    "        model_folder = os.path.join(new_base, f\"img-{i}\", \"model\")\n",
    "        output_folder = os.path.join(new_base, f\"img-{i}\", \"output\")\n",
    "        new_structures.extend([img_folder, model_folder, output_folder])\n",
    "    \n",
    "    # 定義 train-evaluate 資料夾結構\n",
    "    train_evaluate_structure = [\n",
    "        os.path.join(base_dir, \"train-evaluate\", \"img\"),\n",
    "        os.path.join(base_dir, \"train-evaluate\", \"model\"),\n",
    "        os.path.join(base_dir, \"train-evaluate\", \"output\"),\n",
    "    ]\n",
    "    \n",
    "    # 合併所有資料夾結構\n",
    "    all_folders = pretrain_structure + new_structures + train_evaluate_structure\n",
    "    \n",
    "    # 創建資料夾\n",
    "    for folder in all_folders:\n",
    "        try:\n",
    "            os.makedirs(folder, exist_ok=True)\n",
    "            print(f\"已創建或已存在資料夾: {folder}\")\n",
    "        except Exception as e:\n",
    "            print(f\"創建資料夾失敗: {folder}\\n錯誤訊息: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_folder_structure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n",
      "請選擇使用的 Backbone 模型：\n",
      "1. ResNet-18\n",
      "2. MobileNetV3 Small\n",
      "3. EfficientNet-B0\n",
      "選擇的 Backbone：efficientnet\n",
      "可訓練的資料夾:\n",
      "1. ./pretrain\\img\n",
      "2. ./new\\img-1\\img\n",
      "3. ./new\\img-2\\img\n",
      "4. ./new\\img-3\\img\n",
      "5. ./new\\img-4\\img\n",
      "6. ./new\\img-5\\img\n",
      "7. ./train-evaluate\\img\n",
      "選擇的資料來源: ./train-evaluate\\img\n",
      "資料將保存到: 模型->./train-evaluate\\model, 輸出->./train-evaluate\\output\n",
      "掃描圖片資料夾: ./train-evaluate\\img\n",
      "找到 19909 張符合條件的圖片。\n",
      "使用所有符合條件的圖片進行訓練。\n",
      "資料集划分: 訓練集 15927，驗證集 1990，測試集 1992\n",
      "資料已保存為: ./train-evaluate\\model\\train_data_train-evaluate.pt, ./train-evaluate\\model\\valid_data_train-evaluate.pt, ./train-evaluate\\model\\test_data_train-evaluate.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ray03\\AppData\\Local\\Temp\\ipykernel_64256\\1432614330.py:1172: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_path)\n",
      "C:\\Users\\ray03\\AppData\\Local\\Temp\\ipykernel_64256\\1432614330.py:1173: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid_data = torch.load(valid_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "從頭開始訓練模型。\n",
      "Epoch [1/120]\n",
      "train loss: 3.5292, train accu: 0.00%\n",
      "val loss: 3.7019, val accu: 0.00%\n",
      "驗證準確率未提升，耐心計數器: 1/10\n",
      "Epoch [2/120]\n",
      "train loss: 3.6885, train accu: 0.00%\n",
      "val loss: 3.6716, val accu: 0.00%\n",
      "驗證準確率未提升，耐心計數器: 2/10\n",
      "Epoch [3/120]\n",
      "train loss: 3.6590, train accu: 0.00%\n",
      "val loss: 3.6332, val accu: 0.00%\n",
      "驗證準確率未提升，耐心計數器: 3/10\n",
      "Epoch [4/120]\n",
      "train loss: 3.5303, train accu: 0.00%\n",
      "val loss: 3.4159, val accu: 0.00%\n",
      "驗證準確率未提升，耐心計數器: 4/10\n",
      "Epoch [5/120]\n",
      "train loss: 3.3295, train accu: 0.00%\n",
      "val loss: 3.2510, val accu: 0.00%\n",
      "驗證準確率未提升，耐心計數器: 5/10\n",
      "Epoch [6/120]\n",
      "train loss: 3.1833, train accu: 0.01%\n",
      "val loss: 3.1138, val accu: 0.00%\n",
      "驗證準確率未提升，耐心計數器: 6/10\n",
      "Epoch [7/120]\n",
      "train loss: 3.0103, train accu: 0.09%\n",
      "val loss: 2.9557, val accu: 0.05%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [8/120]\n",
      "train loss: 2.7972, train accu: 0.35%\n",
      "val loss: 2.7721, val accu: 0.05%\n",
      "驗證準確率未提升，耐心計數器: 1/10\n",
      "Epoch [9/120]\n",
      "train loss: 2.5863, train accu: 0.71%\n",
      "val loss: 2.5925, val accu: 0.25%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [10/120]\n",
      "train loss: 2.3940, train accu: 1.71%\n",
      "val loss: 2.4680, val accu: 0.30%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [11/120]\n",
      "train loss: 2.2282, train accu: 3.21%\n",
      "val loss: 2.4004, val accu: 0.40%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [12/120]\n",
      "train loss: 2.0639, train accu: 5.18%\n",
      "val loss: 2.3249, val accu: 0.55%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [13/120]\n",
      "train loss: 1.9301, train accu: 8.63%\n",
      "val loss: 2.2311, val accu: 0.90%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [14/120]\n",
      "train loss: 1.7982, train accu: 11.72%\n",
      "val loss: 2.1905, val accu: 1.11%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [15/120]\n",
      "train loss: 1.6830, train accu: 14.55%\n",
      "val loss: 2.1473, val accu: 1.41%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [16/120]\n",
      "train loss: 1.5665, train accu: 19.92%\n",
      "val loss: 2.1724, val accu: 1.81%\n",
      "驗證準確率提升，模型已保存。\n",
      "Epoch [17/120]\n",
      "train loss: 1.4626, train accu: 24.20%\n",
      "val loss: 2.1039, val accu: 1.51%\n",
      "驗證準確率未提升，耐心計數器: 1/10\n",
      "Epoch [18/120]\n",
      "train loss: 1.3753, train accu: 28.81%\n",
      "val loss: 2.1209, val accu: 1.81%\n",
      "驗證準確率未提升，耐心計數器: 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1484\u001b[0m\n\u001b[0;32m   1481\u001b[0m         sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)   \n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1484\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 1206\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1203\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(NEW_DIR, img_num, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;66;03m# 訓練模型並獲取損失和準確率數據\u001b[39;00m\n\u001b[1;32m-> 1206\u001b[0m train_losses, valid_losses, train_accuracies, valid_accuracies \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m   1207\u001b[0m     train_loader,\n\u001b[0;32m   1208\u001b[0m     valid_loader,\n\u001b[0;32m   1209\u001b[0m     save_model_path\u001b[38;5;241m=\u001b[39mmodel_save_path,\n\u001b[0;32m   1210\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m   1211\u001b[0m     backbone\u001b[38;5;241m=\u001b[39mbackbone,\n\u001b[0;32m   1212\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[0;32m   1213\u001b[0m     pretrained_model_path\u001b[38;5;241m=\u001b[39mselected_pretrained_model\n\u001b[0;32m   1214\u001b[0m )\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型已保存為 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;66;03m# 繪製訓練和驗證的損失及準確率曲線\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 746\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_loader, valid_loader, save_model_path, output_dir, backbone, num_epochs, pretrained_model_path)\u001b[0m\n\u001b[0;32m    743\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    745\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 746\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)  \u001b[38;5;66;03m# [seq_len, batch, num_classes]\u001b[39;00m\n\u001b[0;32m    748\u001b[0m input_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(\n\u001b[0;32m    749\u001b[0m     size\u001b[38;5;241m=\u001b[39m(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),),\n\u001b[0;32m    750\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    751\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong\n\u001b[0;32m    752\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    753\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels, input_lengths, label_lengths)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 447\u001b[0m, in \u001b[0;36mCRNN_GRU_EfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 447\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)  \u001b[38;5;66;03m# [W/8, batch, num_classes]\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m conv\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 384\u001b[0m, in \u001b[0;36mEfficientNet_CRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# 編碼器部分\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)  \u001b[38;5;66;03m# [batch, 1280, H/?, W/?]\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     batch, channels, height, width \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;66;03m# 假設下採樣因子為8\u001b[39;00m\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;66;03m# 使用自適應平均池化將高度縮減為1\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[0;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\ray03\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchvision.models as models  # 引入 ResNet18、MobileNetV3 和 EfficientNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 設定隨機種子以確保結果可重現\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 定義圖片的目標大小，所有圖片將縮放到此大小\n",
    "IMAGE_SIZE = (128, 32)  # 根據驗證碼圖片調整大小\n",
    "\n",
    "# 定義圖片所在的資料夾\n",
    "BASE_DIR = \"./\"  # 基礎目錄\n",
    "PRETRAIN_DIR = os.path.join(BASE_DIR, \"pretrain\")\n",
    "PRETRAIN_IMG_DIR = os.path.join(PRETRAIN_DIR, \"img\")\n",
    "NEW_DIR = os.path.join(BASE_DIR, \"new\")\n",
    "TRAIN_EVALUATE_DIR = os.path.join(BASE_DIR, \"train-evaluate\")\n",
    "PREDICT_DIR = os.path.join(BASE_DIR, \"predict\")\n",
    "os.makedirs(PREDICT_DIR, exist_ok=True)\n",
    "\n",
    "# 定義資料保存的資料夾\n",
    "# pretrain, new 和 train-evaluate 下的 model 和 output 資料夾\n",
    "PRETRAIN_MODEL_DIR = os.path.join(PRETRAIN_DIR, \"model\")\n",
    "PRETRAIN_OUTPUT_DIR = os.path.join(PRETRAIN_DIR, \"output\")\n",
    "TRAIN_EVALUATE_MODEL_DIR = os.path.join(TRAIN_EVALUATE_DIR, \"model\")\n",
    "TRAIN_EVALUATE_OUTPUT_DIR = os.path.join(TRAIN_EVALUATE_DIR, \"output\")\n",
    "\n",
    "# 新增：分類模型保存的資料夾\n",
    "CLASSIFIER_DIR = os.path.join(NEW_DIR, \"classifier\")\n",
    "CLASSIFIER_MODEL_DIR = os.path.join(CLASSIFIER_DIR, \"model\")\n",
    "CLASSIFIER_OUTPUT_DIR = os.path.join(CLASSIFIER_DIR, \"output\")\n",
    "os.makedirs(CLASSIFIER_MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(CLASSIFIER_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 定義字母數字列表\n",
    "ALPHA_NUMS = \"abcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "NUM_CLASSES = len(ALPHA_NUMS) + 1  # 加1是為了CTC的blank字符\n",
    "\n",
    "# 定義驗證碼的位數範圍\n",
    "MIN_DIGITS = 4\n",
    "MAX_DIGITS = 6\n",
    "\n",
    "# 檢查設備是否有GPU可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 建立字符與索引的映射\n",
    "char_to_idx = {c: i + 1 for i, c in enumerate(ALPHA_NUMS)}  # 從1開始，0留給blank\n",
    "idx_to_char = {i + 1: c for i, c in enumerate(ALPHA_NUMS)}\n",
    "idx_to_char[0] = ''  # blank字符\n",
    "\n",
    "def image_to_tensor(img):\n",
    "    \"\"\"\n",
    "    將圖片轉換為Tensor\n",
    "    :param img: PIL Image物件\n",
    "    :return: 經過處理的Tensor\n",
    "    \"\"\"\n",
    "    in_img = img.resize(IMAGE_SIZE)\n",
    "    in_img = in_img.convert(\"L\")  # 轉換為灰階圖\n",
    "    arr = np.array(in_img)\n",
    "    t = torch.from_numpy(arr).float()\n",
    "    t = t.unsqueeze(0)  # 增加通道維度 [1, H, W]\n",
    "    t = t / 255.0  # 進行歸一化\n",
    "    return t\n",
    "\n",
    "def code_to_indices(code):\n",
    "    \"\"\"\n",
    "    將驗證碼字串轉換為索引列表\n",
    "    :param code: 驗證碼字串\n",
    "    :return: 索引列表\n",
    "    \"\"\"\n",
    "    return [char_to_idx[c] for c in code.lower() if c in char_to_idx]\n",
    "\n",
    "def prepare_data(image_dir, save_prefix, parent_dir, max_num=None):\n",
    "    \"\"\"\n",
    "    準備訓練資料\n",
    "    :param image_dir: 圖片所在的資料夾\n",
    "    :param save_prefix: 保存資料的前綴（例如 'pretrain' 或 'new_img-1'）\n",
    "    :param parent_dir: 'pretrain', 'new/img-N' 或 'train-evaluate'\n",
    "    :param max_num: 要選擇的最大圖片數量（可選）\n",
    "    :return: (train_path, valid_path, test_path) 或 None\n",
    "    \"\"\"\n",
    "    if parent_dir == \"pretrain\":\n",
    "        model_dir = PRETRAIN_MODEL_DIR\n",
    "        output_dir = PRETRAIN_OUTPUT_DIR\n",
    "    elif parent_dir.startswith(\"new/img-\"):\n",
    "        img_num = parent_dir.split('/')[-1]  # 例如 'img-3'\n",
    "        model_dir = os.path.join(NEW_DIR, img_num, \"model\")\n",
    "        output_dir = os.path.join(NEW_DIR, img_num, \"output\")\n",
    "    elif parent_dir == \"train-evaluate\":\n",
    "        model_dir = TRAIN_EVALUATE_MODEL_DIR\n",
    "        output_dir = TRAIN_EVALUATE_OUTPUT_DIR\n",
    "    else:\n",
    "        print(f\"未知的 parent_dir: {parent_dir}\")\n",
    "        return None\n",
    "\n",
    "    # 創建保存資料的資料夾（自動創建缺失的資料夾）\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"資料將保存到: 模型->{model_dir}, 輸出->{output_dir}\")\n",
    "\n",
    "    print(f\"掃描圖片資料夾: {image_dir}\")\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(image_dir):\n",
    "        for filename in files:\n",
    "            if not (filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".png\")):\n",
    "                continue\n",
    "            path = os.path.join(root, filename)\n",
    "            code = os.path.splitext(filename)[0]\n",
    "            if MIN_DIGITS <= len(code) <= MAX_DIGITS:\n",
    "                if all(c in ALPHA_NUMS for c in code.lower()):\n",
    "                    image_paths.append(path)\n",
    "                    labels.append(code)\n",
    "                else:\n",
    "                    print(f\"跳過包含無效字符的檔案: {filename}\")\n",
    "\n",
    "    print(f\"找到 {len(image_paths)} 張符合條件的圖片。\")\n",
    "\n",
    "    # 檢查是否有資料\n",
    "    if len(image_paths) == 0:\n",
    "        print(\"未找到符合條件的圖片。請檢查圖片資料夾和命名格式。\")\n",
    "        return None\n",
    "\n",
    "    # 如果指定了 max_num，則隨機選擇 max_num 張圖片\n",
    "    if max_num is not None and max_num < len(image_paths):\n",
    "        combined = list(zip(image_paths, labels))\n",
    "        random.shuffle(combined)\n",
    "        combined = combined[:max_num]\n",
    "        image_paths, labels = zip(*combined)\n",
    "        image_paths = list(image_paths)\n",
    "        labels = list(labels)\n",
    "        print(f\"隨機選擇了 {max_num} 張圖片進行訓練。\")\n",
    "    else:\n",
    "        print(\"使用所有符合條件的圖片進行訓練。\")\n",
    "\n",
    "    # 打亂資料\n",
    "    data = list(zip(image_paths, labels))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # 划分資料集\n",
    "    total_samples = len(data)\n",
    "    train_size = int(0.8 * total_samples)\n",
    "    valid_size = int(0.1 * total_samples)\n",
    "    test_size = total_samples - train_size - valid_size\n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    valid_data = data[train_size:train_size + valid_size]\n",
    "    test_data = data[train_size + valid_size:]\n",
    "\n",
    "    print(f\"資料集划分: 訓練集 {len(train_data)}，驗證集 {len(valid_data)}，測試集 {len(test_data)}\")\n",
    "\n",
    "    # 保存資料到相應的資料夾\n",
    "    train_path = os.path.join(model_dir, f\"train_data_{save_prefix}.pt\")\n",
    "    valid_path = os.path.join(model_dir, f\"valid_data_{save_prefix}.pt\")\n",
    "    test_path = os.path.join(model_dir, f\"test_data_{save_prefix}.pt\")\n",
    "\n",
    "    torch.save(train_data, train_path)\n",
    "    torch.save(valid_data, valid_path)\n",
    "    torch.save(test_data, test_path)\n",
    "\n",
    "    print(f\"資料已保存為: {train_path}, {valid_path}, {test_path}\")\n",
    "    return train_path, valid_path, test_path\n",
    "\n",
    "class CaptchaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    驗證碼資料集\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, code = self.data[idx]\n",
    "        try:\n",
    "            with Image.open(path) as img:\n",
    "                image = image_to_tensor(img)\n",
    "        except Exception as e:\n",
    "            print(f\"無法打開圖片 {path}: {e}\")\n",
    "            # 跳過這個樣本，重新隨機選擇一個樣本\n",
    "            return self.__getitem__(random.randint(0, len(self.data) - 1))\n",
    "        label = code_to_indices(code)\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    用於DataLoader的collate_fn，處理不同長度的序列\n",
    "    :param batch: 一批資料\n",
    "    :return: 圖片Tensor，標籤Tensor，標籤長度Tensor\n",
    "    \"\"\"\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "# ------------------- ResNet18、MobileNetV3 Small 和 EfficientNet 的 CRNN 模型 -------------------\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"ResNet 使用的基礎塊\"\"\"\n",
    "    expansion = 1 \n",
    "    def __init__(self, channels_in, channels_out, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, channels_out, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels_out))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(channels_out, channels_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(channels_out))\n",
    "    \n",
    "        self.identity = nn.Sequential()\n",
    "        if stride != 1 or  channels_in != channels_out * self.expansion:\n",
    "            self.identity = nn.Sequential(\n",
    "                nn.Conv2d(channels_in, channels_out * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channels_out * self.expansion))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tmp = self.conv1(x)\n",
    "        tmp = nn.functional.relu(tmp)\n",
    "        tmp = self.conv2(tmp)\n",
    "        tmp += self.identity(x)\n",
    "        y = nn.functional.relu(tmp)\n",
    "        return y\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"自定義的 CNN 模組，基於 BasicBlock 和 ResNet18 的 CNN 部分\"\"\"\n",
    "    def __init__(self, block_type=BasicBlock):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.previous_channels_out = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.previous_channels_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.previous_channels_out))\n",
    "        \n",
    "        self.layer1 = self._make_layer(block_type, channels_out=64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(block_type, channels_out=128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(block_type, channels_out=256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(block_type, channels_out=512, num_blocks=2, stride=2)\n",
    "        \n",
    "    def _make_layer(self, block_type, channels_out, num_blocks, stride):\n",
    "        blocks = []\n",
    "        \n",
    "        blocks.append(block_type(self.previous_channels_out, channels_out, stride))\n",
    "        self.previous_channels_out = channels_out * block_type.expansion\n",
    "        for _ in range(num_blocks-1):\n",
    "            blocks.append(block_type(self.previous_channels_out, self.previous_channels_out, 1))\n",
    "            # 注意這裡不需要再乘以 expansion，因為 channels_out 已經考慮了 expansion\n",
    "        return nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "      \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "class MobileNetV3_CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    基於 MobileNetV3 Small 的 CRNN 模型\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_size=256, num_gru_layers=2, bidirectional=True):\n",
    "        super(MobileNetV3_CRNN, self).__init__()\n",
    "        \n",
    "        # 載入預訓練的 MobileNetV3 Small\n",
    "        mobilenet = models.mobilenet_v3_small(pretrained=0)\n",
    "        \n",
    "        # 修改第一個卷積層以接受單通道輸入\n",
    "        self.encoder = mobilenet.features  # MobileNetV3 Small 的特徵提取部分\n",
    "        self.encoder[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        # 重新初始化修改後的第一層\n",
    "        nn.init.kaiming_normal_(self.encoder[0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "        # 調整下採樣因子：將 stride=2 的層改為 stride=1，減少下採樣\n",
    "        for idx, layer in enumerate(self.encoder):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if layer.stride == (2, 2):\n",
    "                    self.encoder[idx].stride = (1, 1)\n",
    "            elif isinstance(layer, nn.Sequential):\n",
    "                for sub_idx, sub_layer in enumerate(layer):\n",
    "                    if isinstance(sub_layer, nn.Conv2d) and sub_layer.stride == (2, 2):\n",
    "                        layer[sub_idx].stride = (1, 1)\n",
    "        \n",
    "        # 定義 GRU 層，input_size 根據 MobileNetV3 Small 的最後一層特徵維度\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=576,  # MobileNetV3 Small 的最後一層特徵維度\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_gru_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 定義全連接層\n",
    "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 編碼器部分\n",
    "        conv = self.encoder(x)  # [batch, 576, H/8, W/8]\n",
    "        batch, channels, height, width = conv.size()\n",
    "        \n",
    "        # 使用自適應平均池化將高度縮減為1\n",
    "        if height != 1:\n",
    "            conv = F.adaptive_avg_pool2d(conv, (1, width))  # [batch, 576, 1, W/8]\n",
    "        \n",
    "        conv = conv.squeeze(2)  # [batch, 576, W/8]\n",
    "        conv = conv.permute(0, 2, 1)  # [batch, W/8, 576]\n",
    "        \n",
    "        # GRU 層\n",
    "        recurrent, _ = self.gru(conv)  # [batch, W/8, hidden_size * num_directions]\n",
    "        \n",
    "        # 全連接層\n",
    "        output = self.fc(recurrent)    # [batch, W/8, num_classes]\n",
    "        output = output.permute(1, 0, 2)  # [W/8, batch, num_classes]\n",
    "        return output\n",
    "\n",
    "class EfficientNet_CRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    基於 EfficientNet-B0 的 CRNN 模型\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, hidden_size=256, num_gru_layers=2, bidirectional=True):\n",
    "        super(EfficientNet_CRNN, self).__init__()\n",
    "        \n",
    "        # 載入預訓練的 EfficientNet-B0\n",
    "        efficientnet = models.efficientnet_b0(pretrained=0)\n",
    "        \n",
    "        # 修改第一個卷積層以接受單通道輸入\n",
    "        self.encoder = efficientnet.features  # EfficientNet-B0 的特徵提取部分\n",
    "        self.encoder[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        # 重新初始化修改後的第一層\n",
    "        nn.init.kaiming_normal_(self.encoder[0][0].weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "        # 調整下採樣因子：將 stride=2 的層改為 stride=1，減少下採樣\n",
    "        # EfficientNet-B0 的總下採樣因子為 32，目標為 8，因此需要減少多個下採樣層\n",
    "        # 通常，EfficientNet 的 Block 層可以透過修改 stride 來調整\n",
    "        # 以下是一個簡化的示例，實際應根據模型結構進行調整\n",
    "        total_downsampling = 1\n",
    "        for idx, layer in enumerate(self.encoder):\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                if layer.stride == (2, 2):\n",
    "                    self.encoder[idx].stride = (1, 1)\n",
    "                    total_downsampling /= 2\n",
    "            elif isinstance(layer, nn.Sequential):\n",
    "                for sub_idx, sub_layer in enumerate(layer):\n",
    "                    if isinstance(sub_layer, nn.Conv2d) and sub_layer.stride == (2, 2):\n",
    "                        layer[sub_idx].stride = (1, 1)\n",
    "                        total_downsampling /= 2\n",
    "        \n",
    "        # 確保總下採樣因子為8\n",
    "        # EfficientNet-B0 的初始下採樣因子為 32，經過修改後需要為 8\n",
    "        # 需要仔細檢查和調整各層的 stride，這裡假設已經達成目標\n",
    "        \n",
    "        # 定義 GRU 層，input_size 根據 EfficientNet-B0 的最後一層特徵維度\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=1280,  # EfficientNet-B0 的最後一層特徵維度\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_gru_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 定義全連接層\n",
    "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 編碼器部分\n",
    "        conv = self.encoder(x)  # [batch, 1280, H/?, W/?]\n",
    "        batch, channels, height, width = conv.size()\n",
    "        \n",
    "        # 假設下採樣因子為8\n",
    "        # 使用自適應平均池化將高度縮減為1\n",
    "        if height != 1:\n",
    "            conv = F.adaptive_avg_pool2d(conv, (1, width))  # [batch, 1280, 1, W/8]\n",
    "        \n",
    "        conv = conv.squeeze(2)  # [batch, 1280, W/8]\n",
    "        conv = conv.permute(0, 2, 1)  # [batch, W/8, 1280]\n",
    "        \n",
    "        # GRU 層\n",
    "        recurrent, _ = self.gru(conv)  # [batch, W/8, hidden_size * num_directions]\n",
    "        \n",
    "        # 全連接層\n",
    "        output = self.fc(recurrent)    # [batch, W/8, num_classes]\n",
    "        output = output.permute(1, 0, 2)  # [W/8, batch, num_classes]\n",
    "        return output\n",
    "\n",
    "# ------------------- CRNN_GRU 和 CRNN_GRU_MobileNetV3 和 CRNN_GRU_EfficientNet -------------------\n",
    "\n",
    "class CRNN_GRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN_GRU, self).__init__()\n",
    "        self.cnn = CustomCNN()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=512,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)  # [batch, 512, H, W]\n",
    "        _, _, H, W = conv.size()\n",
    "        pool = F.adaptive_avg_pool2d(conv, (1, W))  # 將高度縮減到 1，寬度保持不變\n",
    "        conv = pool.squeeze(2)  # [batch, 512, W]\n",
    "        conv = conv.permute(0, 2, 1)  # [batch, W, 512]\n",
    "\n",
    "        recurrent, _ = self.gru(conv)  # [batch, W, 512]\n",
    "        output = self.fc(recurrent)    # [batch, W, num_classes]\n",
    "        output = output.permute(1, 0, 2)  # [W, batch, num_classes]\n",
    "        return output\n",
    "\n",
    "class CRNN_GRU_MobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN_GRU_MobileNetV3, self).__init__()\n",
    "        self.cnn = MobileNetV3_CRNN(num_classes=num_classes)\n",
    "        # 不再定義 GRU 和 FC 層，因為它們已在 MobileNetV3_CRNN 中定義\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)  # [W/8, batch, num_classes]\n",
    "        return conv\n",
    "\n",
    "class CRNN_GRU_EfficientNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN_GRU_EfficientNet, self).__init__()\n",
    "        self.cnn = EfficientNet_CRNN(num_classes=num_classes)\n",
    "        # 不再定義 GRU 和 FC 層，因為它們已在 EfficientNet_CRNN 中定義\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)  # [W/8, batch, num_classes]\n",
    "        return conv\n",
    "\n",
    "# ------------------- 其餘函數保持不變 -------------------\n",
    "\n",
    "def decode_predictions(preds):\n",
    "    \"\"\"\n",
    "    將模型預測的輸出轉換為字串\n",
    "    :param preds: 模型預測結果\n",
    "    :return: 預測的字串列表\n",
    "    \"\"\"\n",
    "    preds = preds.permute(1, 0, 2)  # [batch, seq_len, num_classes]\n",
    "    preds = torch.argmax(preds, dim=2)  # [batch, seq_len]\n",
    "    preds = preds.cpu().numpy()\n",
    "\n",
    "    decoded_strings = []\n",
    "    for pred in preds:\n",
    "        chars = []\n",
    "        prev_char_idx = None\n",
    "        for idx in pred:\n",
    "            if idx != prev_char_idx and idx != 0:\n",
    "                chars.append(idx_to_char.get(idx, ''))\n",
    "            prev_char_idx = idx\n",
    "        decoded_strings.append(''.join(chars))\n",
    "    return decoded_strings\n",
    "\n",
    "def calculate_accuracy(model, data_loader):\n",
    "    \"\"\"\n",
    "    計算模型在資料集上的準確率\n",
    "    :param model: 訓練好的模型\n",
    "    :param data_loader: 資料加載器\n",
    "    :return: 準確率\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, label_lengths in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)  # [seq_len, batch, num_classes]\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            preds = outputs.detach().cpu()\n",
    "            pred_strings = decode_predictions(preds)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            label_lengths = label_lengths.cpu().numpy()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_count += batch_size\n",
    "\n",
    "            label_strings = []\n",
    "            index = 0\n",
    "            for length in label_lengths:\n",
    "                label = labels[index:index + length]\n",
    "                label_str = ''.join([idx_to_char.get(idx, '') for idx in label])\n",
    "                label_strings.append(label_str)\n",
    "                index += length\n",
    "\n",
    "            for pred_str, label_str in zip(pred_strings, label_strings):\n",
    "                if pred_str == label_str:\n",
    "                    correct_count += 1\n",
    "\n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy\n",
    "\n",
    "# ------------------- 分類模型的定義保持不變 -------------------\n",
    "\n",
    "class ClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ClassifierCNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # [batch, 32, H, W]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # [batch, 32, H/2, W/2]\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # [batch, 64, H/2, W/2]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # [batch, 64, H/4, W/4]\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # [batch, 128, H/4, W/4]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # [batch, 128, H/8, W/8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (IMAGE_SIZE[0] // 8) * (IMAGE_SIZE[1] // 8), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)  # [batch, 128, H/8, W/8]\n",
    "        x = x.view(x.size(0), -1)  # [batch, 128 * H/8 * W/8]\n",
    "        x = self.fc(x)  # [batch, num_classes]\n",
    "        return x\n",
    "\n",
    "def train_classifier():\n",
    "    \"\"\"\n",
    "    訓練分類模型的函數\n",
    "    \"\"\"\n",
    "    # 收集所有 img-N 資料夾\n",
    "    img_folders = [f for f in os.listdir(NEW_DIR) if os.path.isdir(os.path.join(NEW_DIR, f)) and f.startswith(\"img-\")]\n",
    "    if not img_folders:\n",
    "        print(\"沒有找到任何 img-N 資料夾。請先準備資料。\")\n",
    "        return\n",
    "\n",
    "    # 準備訓練資料\n",
    "    train_data = []\n",
    "    labels = []\n",
    "    for idx, folder in enumerate(img_folders):\n",
    "        img_dir = os.path.join(NEW_DIR, folder, \"img\")\n",
    "        # 選取部分圖片\n",
    "        total_images = 0\n",
    "        for root, dirs, files in os.walk(img_dir):\n",
    "            for filename in files:\n",
    "                if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".png\"):\n",
    "                    code = os.path.splitext(filename)[0]\n",
    "                    if MIN_DIGITS <= len(code) <= MAX_DIGITS and all(c in ALPHA_NUMS for c in code.lower()):\n",
    "                        total_images += 1\n",
    "        print(f\"資料夾 '{img_dir}' 中共有 {total_images} 張符合條件的圖片。\")\n",
    "        try:\n",
    "            user_input = input(f\"請輸入要用於訓練分類模型的圖片數量（最大 {total_images}）： \").strip()\n",
    "            desired_num = int(user_input)\n",
    "            if desired_num <= 0 or desired_num > total_images:\n",
    "                print(\"無效的數量。將跳過此資料夾。\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"將從 '{img_dir}' 中隨機選取 {desired_num} 張圖片進行訓練。\")\n",
    "        except ValueError:\n",
    "            print(\"無效的輸入。將跳過此資料夾。\")\n",
    "            continue\n",
    "        \n",
    "        selected_images = []\n",
    "        for root, dirs, files in os.walk(img_dir):\n",
    "            for filename in files:\n",
    "                if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".png\"):\n",
    "                    code = os.path.splitext(filename)[0]\n",
    "                    if MIN_DIGITS <= len(code) <= MAX_DIGITS and all(c in ALPHA_NUMS for c in code.lower()):\n",
    "                        selected_images.append(os.path.join(root, filename))\n",
    "        if len(selected_images) < desired_num:\n",
    "            print(f\"資料夾 '{img_dir}' 中符合條件的圖片少於 {desired_num} 張。將使用所有圖片。\")\n",
    "            desired_num = len(selected_images)\n",
    "        random.shuffle(selected_images)\n",
    "        selected_images = selected_images[:desired_num]\n",
    "        for img_path in selected_images:\n",
    "            train_data.append((img_path, idx))  # 標籤為資料夾的索引\n",
    "            labels.append(idx)\n",
    "    \n",
    "    if not train_data:\n",
    "        print(\"沒有足夠的資料來訓練分類模型。\")\n",
    "        return\n",
    "\n",
    "    # 建立資料集和資料加載器\n",
    "    class ClassifierDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            path, label = self.data[idx]\n",
    "            try:\n",
    "                with Image.open(path) as img:\n",
    "                    image = image_to_tensor(img)\n",
    "            except Exception as e:\n",
    "                print(f\"無法打開圖片 {path}: {e}\")\n",
    "                return self.__getitem__(random.randint(0, len(self.data) - 1))\n",
    "            return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "    classifier_dataset = ClassifierDataset(train_data)\n",
    "    classifier_loader = torch.utils.data.DataLoader(\n",
    "        classifier_dataset, batch_size=32, shuffle=True, collate_fn=lambda batch: (torch.stack([b[0] for b in batch]), torch.stack([b[1] for b in batch]))\n",
    "    )\n",
    "\n",
    "    # 定義分類模型\n",
    "    num_classes = len(img_folders)\n",
    "    classifier_model = ClassifierCNN(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier_model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    # 訓練分類模型\n",
    "    num_epochs = 20\n",
    "    best_accuracy = 0.0\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        classifier_model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in classifier_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc * 100:.2f}%\")\n",
    "\n",
    "        # 儲存最佳模型\n",
    "        if epoch_acc > best_accuracy:\n",
    "            best_accuracy = epoch_acc\n",
    "            torch.save(classifier_model.state_dict(), os.path.join(CLASSIFIER_MODEL_DIR, \"classifier_model.pt\"))\n",
    "            print(\"最佳模型已保存。\")\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    print(f\"分類模型訓練完成，最佳準確率: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "# ------------------- 通用訓練函數 -------------------\n",
    "\n",
    "def train_model(train_loader, valid_loader, save_model_path, output_dir, backbone='resnet18', num_epochs=50, pretrained_model_path=None):\n",
    "    \"\"\"\n",
    "    通用訓練函數\n",
    "    :param train_loader: 訓練資料加載器\n",
    "    :param valid_loader: 驗證資料加載器\n",
    "    :param save_model_path: 模型保存路徑\n",
    "    :param output_dir: 輸出資料夾路徑\n",
    "    :param backbone: 使用的 Backbone ('resnet18'、'mobilenetv3' 或 'efficientnet')\n",
    "    :param num_epochs: 訓練輪數\n",
    "    :param pretrained_model_path: 預訓練模型路徑（可選）\n",
    "    :return: train_losses, valid_losses, train_accuracies, valid_accuracies\n",
    "    \"\"\"\n",
    "    # 根據 Backbone 選擇模型\n",
    "    if backbone == 'resnet18':\n",
    "        model = CRNN_GRU(num_classes=NUM_CLASSES).to(device)\n",
    "    elif backbone == 'mobilenetv3':\n",
    "        model = CRNN_GRU_MobileNetV3(num_classes=NUM_CLASSES).to(device)\n",
    "    elif backbone == 'efficientnet':\n",
    "        model = CRNN_GRU_EfficientNet(num_classes=NUM_CLASSES).to(device)\n",
    "    else:\n",
    "        print(f\"未知的 backbone: {backbone}\")\n",
    "        return\n",
    "\n",
    "    # 如果提供了預訓練模型路徑，且檔案存在，則載入模型權重\n",
    "    if pretrained_model_path and os.path.exists(pretrained_model_path):\n",
    "        model.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n",
    "        print(f\"載入預訓練模型: {pretrained_model_path}\")\n",
    "    else:\n",
    "        print(\"從頭開始訓練模型。\")\n",
    "\n",
    "    # 全面微調：確保所有層參與訓練\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # 使用較小的學習率\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # 使用學習率調度器\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, min_lr=1e-6)\n",
    "    \n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "\n",
    "    best_valid_accuracy = 0.0  # 初始最佳驗證準確率\n",
    "    patience = 10  # Early Stopping的耐心值，即容忍多少個epoch沒有提升\n",
    "    patience_counter = 0  # 記錄驗證準確率沒有改善的epoch次數\n",
    "\n",
    "    # 記錄損失和準確率\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    # 確保保存目錄存在\n",
    "    os.makedirs(os.path.dirname(save_model_path), exist_ok=True)\n",
    "\n",
    "    # 打開CSV檔案以寫入模式\n",
    "    csv_filename = os.path.join(\n",
    "        output_dir,  # 使用相應的 output 資料夾\n",
    "        f'train_val_results_{os.path.splitext(os.path.basename(save_model_path))[0]}.csv'\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(csv_filename), exist_ok=True)\n",
    "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        # 寫入表頭\n",
    "        csvwriter.writerow(['Epoch', 'train loss', 'val loss', 'train accu', 'val accu'])\n",
    "\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for images, labels, label_lengths in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)  # [seq_len, batch, num_classes]\n",
    "\n",
    "                input_lengths = torch.full(\n",
    "                    size=(images.size(0),),\n",
    "                    fill_value=outputs.size(0),\n",
    "                    dtype=torch.long\n",
    "                ).to(device)\n",
    "                loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "\n",
    "                loss.backward()\n",
    "                \n",
    "                # 梯度裁剪以防止梯度爆炸\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "            # 計算訓練集準確率\n",
    "            train_accuracy = calculate_accuracy(model, train_loader)\n",
    "            # 計算驗證集損失和準確率\n",
    "            model.eval()\n",
    "            total_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for images, labels, label_lengths in valid_loader:\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    outputs = model(images)\n",
    "                    input_lengths = torch.full(\n",
    "                        size=(images.size(0),),\n",
    "                        fill_value=outputs.size(0),\n",
    "                        dtype=torch.long\n",
    "                    ).to(device)\n",
    "                    loss = criterion(outputs, labels, input_lengths, label_lengths)\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "            avg_valid_loss = total_loss / len(valid_loader)\n",
    "            valid_accuracy = calculate_accuracy(model, valid_loader)\n",
    "\n",
    "            # 記錄每個epoch的損失和準確率\n",
    "            train_losses.append(avg_train_loss)\n",
    "            valid_losses.append(avg_valid_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "            # 將結果寫入CSV檔案\n",
    "            csvwriter.writerow([epoch, avg_train_loss, avg_valid_loss, train_accuracy, valid_accuracy])\n",
    "\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "            print(f\"train loss: {avg_train_loss:.4f}, train accu: {train_accuracy * 100:.2f}%\")\n",
    "            print(f\"val loss: {avg_valid_loss:.4f}, val accu: {valid_accuracy * 100:.2f}%\")\n",
    "            scheduler.step(avg_valid_loss)\n",
    "\n",
    "            # Early Stopping檢查（基於驗證集準確率）\n",
    "            if valid_accuracy > best_valid_accuracy:\n",
    "                best_valid_accuracy = valid_accuracy\n",
    "                patience_counter = 0  # 重置耐心計數器\n",
    "                torch.save(model.state_dict(), save_model_path)\n",
    "                print(\"驗證準確率提升，模型已保存。\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"驗證準確率未提升，耐心計數器: {patience_counter}/{patience}\")\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"因為驗證準確率長期未提升，提前停止訓練。\")\n",
    "                break  # 提前停止訓練\n",
    "\n",
    "    return train_losses, valid_losses, train_accuracies, valid_accuracies\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies):\n",
    "    \"\"\"\n",
    "    繪製訓練和驗證的損失及準確率曲線\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 損失曲線\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')  \n",
    "    plt.plot(epochs, valid_losses, 'orange', label='Validation Loss')  \n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 準確率曲線\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, 'g-', label='Train Accuracy')  \n",
    "    plt.plot(epochs, valid_accuracies, 'r-', label='Validation Accuracy')  \n",
    "    plt.title('Train and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_from_csv(csv_path):\n",
    "    \"\"\"\n",
    "    從CSV文件中讀取訓練和驗證的損失及準確率，並繪製曲線\n",
    "    \"\"\"\n",
    "    epochs = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "\n",
    "    with open(csv_path, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            epochs.append(int(row['Epoch']))\n",
    "            train_losses.append(float(row['train loss']))\n",
    "            valid_losses.append(float(row['val loss']))\n",
    "            train_accuracies.append(float(row['train accu']))\n",
    "            valid_accuracies.append(float(row['val accu']))\n",
    "\n",
    "    plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies)\n",
    "\n",
    "def evaluate_model(save_model_path, test_data_path, backbone='resnet18'):\n",
    "    \"\"\"\n",
    "    評估模型並生成混淆矩陣\n",
    "    :param save_model_path: 模型檔案路徑\n",
    "    :param test_data_path: 測試資料集路徑\n",
    "    :param backbone: 使用的 Backbone ('resnet18'、'mobilenetv3' 或 'efficientnet')\n",
    "    \"\"\"\n",
    "    # 載入測試資料\n",
    "    if not os.path.exists(test_data_path):\n",
    "        print(f\"測試資料檔案 '{test_data_path}' 不存在。請先準備資料。\")\n",
    "        return\n",
    "\n",
    "    test_data = torch.load(test_data_path)\n",
    "    test_dataset = CaptchaDataset(test_data)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # 根據 Backbone 選擇模型\n",
    "    if backbone == 'resnet18':\n",
    "        model = CRNN_GRU(num_classes=NUM_CLASSES).to(device)\n",
    "    elif backbone == 'mobilenetv3':\n",
    "        model = CRNN_GRU_MobileNetV3(num_classes=NUM_CLASSES).to(device)\n",
    "    elif backbone == 'efficientnet':\n",
    "        model = CRNN_GRU_EfficientNet(num_classes=NUM_CLASSES).to(device)\n",
    "    else:\n",
    "        print(f\"未知的 backbone: {backbone}\")\n",
    "        return\n",
    "\n",
    "    # 載入模型\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "        print(f\"已載入模型: {save_model_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"模型檔案 '{save_model_path}' 未找到。請先訓練模型。\")\n",
    "        return\n",
    "    model.eval()\n",
    "\n",
    "    total_sequences = 0  # 總序列數\n",
    "    correct_sequences = 0  # 完全正確的序列數\n",
    "\n",
    "    total_chars = 0  # 總字元數\n",
    "    correct_chars = 0  # 正確字元數\n",
    "\n",
    "    all_true_chars = []\n",
    "    all_pred_chars = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, label_lengths in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)  # [seq_len, batch, num_classes]\n",
    "            outputs = outputs.log_softmax(2)\n",
    "            preds = outputs.detach().cpu()\n",
    "            pred_strings = decode_predictions(preds)\n",
    "\n",
    "            labels = labels.cpu().numpy()\n",
    "            label_lengths = label_lengths.cpu().numpy()\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_sequences += batch_size  # 增加序列計數\n",
    "\n",
    "            label_strings = []\n",
    "            index = 0\n",
    "            for length in label_lengths:\n",
    "                label = labels[index:index + length]\n",
    "                label_str = ''.join([idx_to_char.get(idx, '') for idx in label])\n",
    "                label_strings.append(label_str)\n",
    "                total_chars += length  # 增加字符計數\n",
    "                index += length\n",
    "\n",
    "            for pred_str, label_str in zip(pred_strings, label_strings):\n",
    "                if pred_str == label_str:\n",
    "                    correct_sequences += 1  # 完全正確的序列計數\n",
    "\n",
    "                # 逐字符比較，計算字符準確率\n",
    "                min_len = min(len(pred_str), len(label_str))\n",
    "                for i in range(min_len):\n",
    "                    if pred_str[i] == label_str[i]:\n",
    "                        correct_chars += 1  # 增加正確字符計數\n",
    "                    all_true_chars.append(label_str[i])\n",
    "                    all_pred_chars.append(pred_str[i])\n",
    "\n",
    "    # 計算準確率\n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    seq_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "\n",
    "    print(f\"字符級準確率: {char_accuracy * 100:.2f}% (正確: {correct_chars}/{total_chars})\")\n",
    "    print(f\"序列級準確率: {seq_accuracy * 100:.2f}% (正確: {correct_sequences}/{total_sequences})\")\n",
    "\n",
    "    # 生成混淆矩陣並轉換為百分比\n",
    "    cm = confusion_matrix(all_true_chars, all_pred_chars, labels=list(ALPHA_NUMS))\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 將每行轉換為百分比\n",
    "\n",
    "    # 繪製混淆矩陣\n",
    "    plt.figure(figsize=(20, 14))  # 圖形大小\n",
    "    sns.heatmap(\n",
    "        cm_percentage,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='Blues',\n",
    "        xticklabels=list(ALPHA_NUMS),\n",
    "        yticklabels=list(ALPHA_NUMS),\n",
    "        annot_kws={\"size\": 6},  # 註釋字體大小\n",
    "        cbar_kws={\"shrink\": 0.8}  # 顏色條\n",
    "    )\n",
    "    plt.xlabel('Predicted Characters ', fontsize=16)  # 字體\n",
    "    plt.ylabel('True Characters', fontsize=16)\n",
    "    plt.title('Confusion Matrix (Percentage)', fontsize=18)\n",
    "    plt.xticks(fontsize=14, rotation=0)  # 旋轉x軸標籤 字體大小\n",
    "    plt.yticks(fontsize=14, rotation=0)   # 旋轉y軸標籤 字體大小\n",
    "    plt.tight_layout()\n",
    "    plt.show()  # 顯示混淆矩陣圖\n",
    "\n",
    "    print(\"混淆矩陣已顯示。\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    主函數，處理不同的操作模式\n",
    "    \"\"\"\n",
    "    operation = input(\"請輸入模式 (prepare|train|train_classifier|evaluate|predict|plot): \").strip().lower()\n",
    "\n",
    "    if operation == \"prepare\":\n",
    "        # 收集所有可準備的資料夾選項\n",
    "        prepare_options = []\n",
    "\n",
    "        # 選項 1: pretrain/img\n",
    "        prepare_options.append({\n",
    "            \"path\": PRETRAIN_IMG_DIR,\n",
    "            \"save_prefix\": \"pretrain\",\n",
    "            \"parent_dir\": \"pretrain\"\n",
    "        })\n",
    "\n",
    "        # 選項 2-N: new/img-N/img\n",
    "        img_folders = [f for f in os.listdir(NEW_DIR) if os.path.isdir(os.path.join(NEW_DIR, f)) and f.startswith(\"img-\")]\n",
    "        if not img_folders:\n",
    "            print(\"尚未建立任何 img-N 資料夾。\")\n",
    "        else:\n",
    "            for folder in img_folders:\n",
    "                prepare_options.append({\n",
    "                    \"path\": os.path.join(NEW_DIR, folder, \"img\"),\n",
    "                    \"save_prefix\": f\"new_{folder}\",\n",
    "                    \"parent_dir\": f\"new/{folder}\"\n",
    "                })\n",
    "\n",
    "        # 選項 N+1: train-evaluate/img\n",
    "        prepare_options.append({\n",
    "            \"path\": os.path.join(TRAIN_EVALUATE_DIR, \"img\"),\n",
    "            \"save_prefix\": \"train-evaluate\",\n",
    "            \"parent_dir\": \"train-evaluate\"\n",
    "        })\n",
    "\n",
    "        # 顯示所有選項\n",
    "        print(\"可準備的資料夾:\")\n",
    "        for idx, option in enumerate(prepare_options, 1):\n",
    "            print(f\"{idx}. {option['path']}\")\n",
    "\n",
    "        # 輸入選擇\n",
    "        try:\n",
    "            source_choice = int(input(f\"請選擇要 prepare 的資料夾編號（1-{len(prepare_options)}）： \").strip()) - 1\n",
    "            if source_choice < 0 or source_choice >= len(prepare_options):\n",
    "                raise ValueError\n",
    "            selected_option = prepare_options[source_choice]\n",
    "            selected_dir = selected_option[\"path\"]\n",
    "            save_prefix = selected_option[\"save_prefix\"]\n",
    "            parent_dir = selected_option[\"parent_dir\"]\n",
    "            print(f\"選擇的資料來源: {selected_dir}\")\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"無效的選擇。\")\n",
    "            return\n",
    "\n",
    "        # 準備資料\n",
    "        prepare_result = prepare_data(selected_dir, save_prefix, parent_dir)\n",
    "        if prepare_result:\n",
    "            print(\"資料準備完成。\")\n",
    "\n",
    "    elif operation == \"train\":\n",
    "        # 選擇是否使用預訓練模型\n",
    "        use_pretrained = input(\"是否使用預訓練模型進行訓練？ (y/n)： \").strip().lower()\n",
    "        if use_pretrained == 'y':\n",
    "            # 從 pretrain 資料夾中選擇預訓練模型\n",
    "            pretrained_models = [f for f in os.listdir(PRETRAIN_MODEL_DIR) if f.startswith(\"crnn_model\") and f.endswith(\".pt\")]\n",
    "            if not pretrained_models:\n",
    "                print(\"在 pretrain/model 資料夾中未找到任何預訓練模型。請先訓練或準備預訓練模型。\")\n",
    "                return\n",
    "            print(\"可用的預訓練模型:\")\n",
    "            for idx, file in enumerate(pretrained_models, 1):\n",
    "                path = os.path.join(PRETRAIN_MODEL_DIR, file)\n",
    "                print(f\"{idx}. {path}\")\n",
    "            try:\n",
    "                model_choice = int(input(f\"請選擇要載入的預訓練模型編號（1-{len(pretrained_models)}）： \").strip()) - 1\n",
    "                if model_choice < 0 or model_choice >= len(pretrained_models):\n",
    "                    raise ValueError\n",
    "                selected_pretrained_model = os.path.join(PRETRAIN_MODEL_DIR, pretrained_models[model_choice])\n",
    "            except (ValueError, IndexError):\n",
    "                print(\"無效的選擇。\")\n",
    "                return\n",
    "        else:\n",
    "            selected_pretrained_model = None\n",
    "\n",
    "        # 選擇使用的 Backbone\n",
    "        print(\"請選擇使用的 Backbone 模型：\")\n",
    "        print(\"1. ResNet-18\")\n",
    "        print(\"2. MobileNetV3 Small\")\n",
    "        print(\"3. EfficientNet-B0\")\n",
    "        try:\n",
    "            backbone_choice = int(input(\"請輸入選項編號（1、2 或 3）： \").strip())\n",
    "            if backbone_choice == 1:\n",
    "                backbone = 'resnet18'\n",
    "            elif backbone_choice == 2:\n",
    "                backbone = 'mobilenetv3'\n",
    "            elif backbone_choice == 3:\n",
    "                backbone = 'efficientnet'\n",
    "            else:\n",
    "                raise ValueError\n",
    "            print(f\"選擇的 Backbone：{backbone}\")\n",
    "        except ValueError:\n",
    "            print(\"無效的選擇。\")\n",
    "            return\n",
    "\n",
    "        # 列出可訓練的資料夾\n",
    "        train_options = []\n",
    "\n",
    "        # 選項 1: pretrain/img\n",
    "        train_options.append({\n",
    "            \"path\": PRETRAIN_IMG_DIR,\n",
    "            \"save_prefix\": \"pretrain\",\n",
    "            \"parent_dir\": \"pretrain\"\n",
    "        })\n",
    "\n",
    "        # 選項 2-N: new/img-N/img\n",
    "        img_folders = [f for f in os.listdir(NEW_DIR) if os.path.isdir(os.path.join(NEW_DIR, f)) and f.startswith(\"img-\")]\n",
    "        if not img_folders:\n",
    "            print(\"沒有找到任何 img-N 資料夾。請先準備資料。\")\n",
    "            return\n",
    "        else:\n",
    "            for folder in img_folders:\n",
    "                train_options.append({\n",
    "                    \"path\": os.path.join(NEW_DIR, folder, \"img\"),\n",
    "                    \"save_prefix\": f\"new_{folder}\",\n",
    "                    \"parent_dir\": f\"new/{folder}\"\n",
    "                })\n",
    "\n",
    "        # 選項 N+1: train-evaluate/img\n",
    "        train_options.append({\n",
    "            \"path\": os.path.join(TRAIN_EVALUATE_DIR, \"img\"),\n",
    "            \"save_prefix\": \"train-evaluate\",\n",
    "            \"parent_dir\": \"train-evaluate\"\n",
    "        })\n",
    "\n",
    "        # 顯示所有選項\n",
    "        print(\"可訓練的資料夾:\")\n",
    "        for idx, option in enumerate(train_options, 1):\n",
    "            print(f\"{idx}. {option['path']}\")\n",
    "\n",
    "        # 輸入選擇\n",
    "        try:\n",
    "            train_choice = int(input(f\"請選擇要訓練的資料夾編號（1-{len(train_options)}）： \").strip()) - 1\n",
    "            if train_choice < 0 or train_choice >= len(train_options):\n",
    "                raise ValueError\n",
    "            selected_train_option = train_options[train_choice]\n",
    "            selected_dir = selected_train_option[\"path\"]\n",
    "            save_prefix = selected_train_option[\"save_prefix\"]\n",
    "            parent_dir = selected_train_option[\"parent_dir\"]\n",
    "            print(f\"選擇的資料來源: {selected_dir}\")\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"無效的選擇。\")\n",
    "            return\n",
    "\n",
    "        # 如果選擇的是 'new/img-N'，則詢問要訓練的圖片數量\n",
    "        max_num = None\n",
    "        if parent_dir.startswith(\"new/img-\"):\n",
    "            img_num = parent_dir.split('/')[-1]  # 例如 'img-3'\n",
    "            total_images = 0\n",
    "            for root, dirs, files in os.walk(selected_dir):\n",
    "                for filename in files:\n",
    "                    if filename.lower().endswith(\".jpg\") or filename.lower().endswith(\".png\"):\n",
    "                        code = os.path.splitext(filename)[0]\n",
    "                        if MIN_DIGITS <= len(code) <= MAX_DIGITS and all(c in ALPHA_NUMS for c in code.lower()):\n",
    "                            total_images += 1\n",
    "            print(f\"資料夾 '{selected_dir}' 中共有 {total_images} 張符合條件的圖片。\")\n",
    "            try:\n",
    "                user_input = input(f\"請輸入要用於訓練的圖片數量（最大 {total_images}，留空則使用所有）： \").strip()\n",
    "                if user_input:\n",
    "                    desired_num = int(user_input)\n",
    "                    if desired_num <= 0 or desired_num > total_images:\n",
    "                        print(\"無效的數量。將使用所有圖片。\")\n",
    "                    else:\n",
    "                        max_num = desired_num\n",
    "                        print(f\"將隨機選擇 {max_num} 張圖片進行訓練。\")\n",
    "            except ValueError:\n",
    "                print(\"無效的輸入。將使用所有圖片。\")\n",
    "\n",
    "        # 準備資料\n",
    "        prepare_result = prepare_data(selected_dir, save_prefix, parent_dir, max_num=max_num)\n",
    "        if prepare_result is None:\n",
    "            print(\"資料準備失敗，訓練終止。\")\n",
    "            return\n",
    "\n",
    "        train_path, valid_path, _ = prepare_result  # 測試集路徑在訓練中不需要\n",
    "\n",
    "        # 創建資料集和資料加載器\n",
    "        train_data = torch.load(train_path)\n",
    "        valid_data = torch.load(valid_path)\n",
    "\n",
    "        train_dataset = CaptchaDataset(train_data)\n",
    "        valid_dataset = CaptchaDataset(valid_data)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    "        )\n",
    "        valid_loader = torch.utils.data.DataLoader(\n",
    "            valid_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "        # 定義模型保存路徑\n",
    "        if parent_dir == \"pretrain\":\n",
    "            model_save_filename = f\"crnn_model-{backbone}-gru_pretrain.pt\"\n",
    "            model_save_path = os.path.join(PRETRAIN_MODEL_DIR, model_save_filename)\n",
    "            output_dir = PRETRAIN_OUTPUT_DIR\n",
    "        elif parent_dir == \"train-evaluate\":\n",
    "            model_save_filename = f\"crnn_model-{backbone}-gru_train-evaluate.pt\"\n",
    "            model_save_path = os.path.join(TRAIN_EVALUATE_MODEL_DIR, model_save_filename)\n",
    "            output_dir = TRAIN_EVALUATE_OUTPUT_DIR\n",
    "        else:\n",
    "            # 將模型保存到 'new/img-N/model' 資料夾\n",
    "            img_num = parent_dir.split('/')[-1]  # 例如 'img-3'\n",
    "            model_save_filename = f\"crnn_model-{backbone}-gru_{img_num}.pt\"\n",
    "            model_save_path = os.path.join(NEW_DIR, img_num, \"model\", model_save_filename)\n",
    "            output_dir = os.path.join(NEW_DIR, img_num, \"output\")\n",
    "\n",
    "            # 確保模型保存資料夾和輸出資料夾存在\n",
    "            os.makedirs(os.path.join(NEW_DIR, img_num, \"model\"), exist_ok=True)\n",
    "            os.makedirs(os.path.join(NEW_DIR, img_num, \"output\"), exist_ok=True)\n",
    "\n",
    "        # 訓練模型並獲取損失和準確率數據\n",
    "        train_losses, valid_losses, train_accuracies, valid_accuracies = train_model(\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            save_model_path=model_save_path,\n",
    "            output_dir=output_dir,\n",
    "            backbone=backbone,\n",
    "            num_epochs=120,\n",
    "            pretrained_model_path=selected_pretrained_model\n",
    "        )\n",
    "        print(f\"模型已保存為 '{model_save_path}'\")\n",
    "\n",
    "        # 繪製訓練和驗證的損失及準確率曲線\n",
    "        plot_training_curves(train_losses, valid_losses, train_accuracies, valid_accuracies)\n",
    "\n",
    "    elif operation == \"train_classifier\":\n",
    "        \"\"\"\n",
    "        新增的分類模型訓練模式\n",
    "        \"\"\"\n",
    "        train_classifier()\n",
    "\n",
    "    elif operation == \"evaluate\":\n",
    "        # 收集所有可評估的資料夾選項\n",
    "        evaluate_options = []\n",
    "\n",
    "        # 選項 1: pretrain/img\n",
    "        evaluate_options.append({\n",
    "            \"data_dir\": PRETRAIN_IMG_DIR,\n",
    "            \"model_dir\": PRETRAIN_MODEL_DIR,\n",
    "            \"save_prefix\": \"pretrain\"\n",
    "        })\n",
    "\n",
    "        # 選項 2-N: new/img-N/img\n",
    "        img_folders = [f for f in os.listdir(NEW_DIR) if os.path.isdir(os.path.join(NEW_DIR, f)) and f.startswith(\"img-\")]\n",
    "        if not img_folders:\n",
    "            print(\"尚未建立任何 img-N 資料夾。\")\n",
    "        else:\n",
    "            for folder in img_folders:\n",
    "                evaluate_options.append({\n",
    "                    \"data_dir\": os.path.join(NEW_DIR, folder, \"img\"),\n",
    "                    \"model_dir\": os.path.join(NEW_DIR, folder, \"model\"),\n",
    "                    \"save_prefix\": f\"new_{folder}\"\n",
    "                })\n",
    "\n",
    "        # 選項 N+1: train-evaluate/img\n",
    "        evaluate_options.append({\n",
    "            \"data_dir\": os.path.join(TRAIN_EVALUATE_DIR, \"img\"),\n",
    "            \"model_dir\": TRAIN_EVALUATE_MODEL_DIR,\n",
    "            \"save_prefix\": \"train-evaluate\"\n",
    "        })\n",
    "\n",
    "        # 檢查是否有可評估的選項\n",
    "        if not evaluate_options:\n",
    "            print(\"沒有可評估的資料夾。請先準備資料。\")\n",
    "            return\n",
    "\n",
    "        # 顯示所有選項\n",
    "        print(\"可評估的資料夾:\")\n",
    "        for idx, option in enumerate(evaluate_options, 1):\n",
    "            print(f\"{idx}. {option['data_dir']}\")\n",
    "\n",
    "        # 輸入選擇\n",
    "        try:\n",
    "            evaluate_choice = int(input(f\"請選擇要評估的資料夾編號（1-{len(evaluate_options)}）： \").strip()) - 1\n",
    "            if evaluate_choice < 0 or evaluate_choice >= len(evaluate_options):\n",
    "                raise ValueError\n",
    "            selected_evaluate_option = evaluate_options[evaluate_choice]\n",
    "            data_dir = selected_evaluate_option[\"data_dir\"]\n",
    "            model_dir = selected_evaluate_option[\"model_dir\"]\n",
    "            save_prefix = selected_evaluate_option[\"save_prefix\"]\n",
    "            print(f\"選擇的資料來源: {data_dir}\")\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"無效的選擇。\")\n",
    "            return\n",
    "\n",
    "        # 檢查模型資料夾是否存在\n",
    "        if not os.path.exists(model_dir):\n",
    "            print(f\"模型資料夾 '{model_dir}' 不存在。請先訓練模型。\")\n",
    "            return\n",
    "\n",
    "        # 列出所有可用的模型文件\n",
    "        model_files = [f for f in os.listdir(model_dir) if f.startswith(\"crnn_model\") and f.endswith(\".pt\")]\n",
    "        if not model_files:\n",
    "            print(f\"在 '{model_dir}' 資料夾中未找到任何模型文件。請先訓練模型。\")\n",
    "            return\n",
    "\n",
    "        # 顯示可用的模型文件\n",
    "        print(\"可用的模型:\")\n",
    "        for idx, file in enumerate(model_files, 1):\n",
    "            path = os.path.join(model_dir, file)\n",
    "            print(f\"{idx}. {path}\")\n",
    "        try:\n",
    "            model_choice = int(input(f\"請選擇要使用的模型編號（1-{len(model_files)}）： \").strip()) - 1\n",
    "            if model_choice < 0 or model_choice >= len(model_files):\n",
    "                raise ValueError\n",
    "            selected_model_file = model_files[model_choice]\n",
    "            selected_model_path = os.path.join(model_dir, selected_model_file)\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"無效的選擇。\")\n",
    "            return\n",
    "\n",
    "        # 根據模型文件名稱判斷 Backbone\n",
    "        if 'resnet18' in selected_model_file:\n",
    "            backbone = 'resnet18'\n",
    "        elif 'mobilenetv3' in selected_model_file:\n",
    "            backbone = 'mobilenetv3'\n",
    "        elif 'efficientnet' in selected_model_file:\n",
    "            backbone = 'efficientnet'\n",
    "        else:\n",
    "            print(\"無法識別模型的 Backbone 類型。請確認模型命名規則。\")\n",
    "            return\n",
    "\n",
    "        # 準備測試資料路徑\n",
    "        test_data_path = os.path.join(model_dir, f\"test_data_{save_prefix}.pt\")\n",
    "        if not os.path.exists(test_data_path):\n",
    "            print(f\"測試資料檔案 '{test_data_path}' 不存在。請先準備資料。\")\n",
    "            return\n",
    "\n",
    "        # 評估模型，傳遞 Backbone 參數\n",
    "        evaluate_model(selected_model_path, test_data_path, backbone=backbone)\n",
    "\n",
    "    elif operation == \"predict\":\n",
    "        \"\"\"\n",
    "        修改後的預測模式，批次預測 'predict' 資料夾中的所有圖片，先分類再預測\n",
    "        \"\"\"\n",
    "        # 檢查並獲取預測資料夾\n",
    "        if not os.path.exists(PREDICT_DIR):\n",
    "            print(f\"預測資料夾 '{PREDICT_DIR}' 不存在。\")\n",
    "            return\n",
    "\n",
    "        # 列出預測資料夾中的所有圖片\n",
    "        image_files = [f for f in os.listdir(PREDICT_DIR) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        if not image_files:\n",
    "            print(f\"預測資料夾 '{PREDICT_DIR}' 中沒有任何圖片。\")\n",
    "            return\n",
    "\n",
    "        # 載入分類模型\n",
    "        classifier_model_path = os.path.join(CLASSIFIER_MODEL_DIR, \"classifier_model.pt\")\n",
    "        if not os.path.exists(classifier_model_path):\n",
    "            print(f\"分類模型 '{classifier_model_path}' 不存在。請先訓練分類模型。\")\n",
    "            return\n",
    "\n",
    "        # 獲取 img-N 資料夾列表\n",
    "        img_folders = [f for f in os.listdir(NEW_DIR) if os.path.isdir(os.path.join(NEW_DIR, f)) and f.startswith(\"img-\")]\n",
    "        if not img_folders:\n",
    "            print(\"沒有找到任何 img-N 資料夾。\")\n",
    "            return\n",
    "        num_classes_classifier = len(img_folders)\n",
    "\n",
    "        classifier_model = ClassifierCNN(num_classes=num_classes_classifier).to(device)\n",
    "        classifier_model.load_state_dict(torch.load(classifier_model_path, map_location=device))\n",
    "        classifier_model.eval()\n",
    "\n",
    "        # 開啟一個CSV文件來保存預測結果\n",
    "        predictions_csv = os.path.join(PREDICT_DIR, \"predictions.csv\")\n",
    "        with open(predictions_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            csvwriter = csv.writer(csvfile)\n",
    "            csvwriter.writerow(['Image', 'Source Folder', 'Predicted CAPTCHA'])\n",
    "\n",
    "            # 批次預測所有圖片\n",
    "            for image_file in image_files:\n",
    "                image_path = os.path.join(PREDICT_DIR, image_file)\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        image_tensor = image_to_tensor(img).unsqueeze(0).to(device)  # [1, 1, H, W]\n",
    "                except Exception as e:\n",
    "                    print(f\"無法打開圖片 {image_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # 使用分類模型預測圖片來源\n",
    "                with torch.no_grad():\n",
    "                    outputs = classifier_model(image_tensor)  # [1, num_classes]\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    predicted_class = predicted.item()\n",
    "\n",
    "                # 獲取分類結果對應的 img-N 資料夾\n",
    "                if predicted_class >= len(img_folders):\n",
    "                    print(f\"分類結果超出範圍 for image {image_file}.\")\n",
    "                    continue\n",
    "                source_folder = img_folders[predicted_class]\n",
    "                print(f\"圖片 {image_file} 分類結果：來自 '{source_folder}' 資料夾。\")\n",
    "\n",
    "                # 載入對應的 OCR 模型\n",
    "                # 根據 Backbone 的不同選擇不同的模型類別\n",
    "                # 假設模型命名為 crnn_model-{backbone}-gru_{img_num}.pt\n",
    "                ocr_model_files = [f for f in os.listdir(os.path.join(NEW_DIR, source_folder, \"model\")) if f.startswith(\"crnn_model\") and f.endswith(\".pt\")]\n",
    "                if not ocr_model_files:\n",
    "                    print(f\"OCR 模型資料夾 '{os.path.join(NEW_DIR, source_folder, 'model')}' 中未找到任何模型文件。請先訓練 OCR 模型。\")\n",
    "                    continue\n",
    "                # 假設只有一個模型文件\n",
    "                ocr_model_filename = ocr_model_files[0]\n",
    "                ocr_model_path = os.path.join(NEW_DIR, source_folder, \"model\", ocr_model_filename)\n",
    "                if not os.path.exists(ocr_model_path):\n",
    "                    print(f\"OCR 模型 '{ocr_model_path}' 不存在。請先訓練 OCR 模型。\")\n",
    "                    continue\n",
    "\n",
    "                # 判斷 OCR 模型的 Backbone\n",
    "                if 'resnet18' in ocr_model_filename:\n",
    "                    ocr_backbone = 'resnet18'\n",
    "                    ocr_model = CRNN_GRU(num_classes=NUM_CLASSES).to(device)\n",
    "                elif 'mobilenetv3' in ocr_model_filename:\n",
    "                    ocr_backbone = 'mobilenetv3'\n",
    "                    ocr_model = CRNN_GRU_MobileNetV3(num_classes=NUM_CLASSES).to(device)\n",
    "                elif 'efficientnet' in ocr_model_filename:\n",
    "                    ocr_backbone = 'efficientnet'\n",
    "                    ocr_model = CRNN_GRU_EfficientNet(num_classes=NUM_CLASSES).to(device)\n",
    "                else:\n",
    "                    print(f\"OCR 模型 '{ocr_model_filename}' 的 Backbone 未知。請確認模型命名規則。\")\n",
    "                    continue\n",
    "\n",
    "                ocr_model.load_state_dict(torch.load(ocr_model_path, map_location=device))\n",
    "                ocr_model.eval()\n",
    "\n",
    "                # 預測驗證碼內容\n",
    "                with torch.no_grad():\n",
    "                    ocr_output = ocr_model(image_tensor)  # [seq_len, batch, num_classes]\n",
    "                    ocr_output = ocr_output.log_softmax(2)\n",
    "                    preds = ocr_output.detach().cpu()\n",
    "                    pred_strings = decode_predictions(preds)\n",
    "\n",
    "                # 輸出並保存預測結果\n",
    "                predicted_captcha = pred_strings[0]\n",
    "                print(f\"圖片 {image_file} 預測的驗證碼內容：{predicted_captcha}\")\n",
    "                csvwriter.writerow([image_file, source_folder, predicted_captcha])\n",
    "\n",
    "        print(f\"所有預測結果已保存至 '{predictions_csv}'\")\n",
    "\n",
    "    elif operation == \"plot\":\n",
    "        # 列出所有 CSV 結果文件\n",
    "        # 包含 pretrain/output、new/img-N/output 和 train-evaluate/output 資料夾\n",
    "        evaluate_options = []\n",
    "\n",
    "        # 選項 1: pretrain/output\n",
    "        evaluate_options.append(os.path.join(PRETRAIN_OUTPUT_DIR))\n",
    "\n",
    "        # 選項 2-N: new/img-N/output\n",
    "        img_folders = [f for f in os.listdir(NEW_DIR) if os.path.isdir(os.path.join(NEW_DIR, f)) and f.startswith(\"img-\")]\n",
    "        for folder in img_folders:\n",
    "            evaluate_options.append(os.path.join(NEW_DIR, folder, \"output\"))\n",
    "\n",
    "        # 選項 N+1: train-evaluate/output\n",
    "        evaluate_options.append(os.path.join(TRAIN_EVALUATE_OUTPUT_DIR))\n",
    "\n",
    "        # 收集所有 CSV 文件\n",
    "        available_csvs = []\n",
    "        for output_dir in evaluate_options:\n",
    "            if os.path.exists(output_dir):\n",
    "                available_csvs += [\n",
    "                    os.path.join(output_dir, f) \n",
    "                    for f in os.listdir(output_dir) \n",
    "                    if f.startswith(\"train_val_results\") and f.endswith(\".csv\")\n",
    "                ]\n",
    "\n",
    "        if not available_csvs:\n",
    "            print(\"沒有找到任何訓練結果的 CSV 文件。請先訓練模型。\")\n",
    "            return\n",
    "\n",
    "        # 顯示可用的 CSV 文件\n",
    "        print(\"可用的訓練結果 CSV 文件:\")\n",
    "        for idx, file in enumerate(available_csvs, 1):\n",
    "            print(f\"{idx}. {file}\")\n",
    "        try:\n",
    "            csv_choice = int(input(f\"請選擇要繪製的 CSV 文件編號（1-{len(available_csvs)}）： \").strip()) - 1\n",
    "            if csv_choice < 0 or csv_choice >= len(available_csvs):\n",
    "                raise ValueError\n",
    "            selected_csv_file = available_csvs[csv_choice]\n",
    "            selected_csv_path = selected_csv_file\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"無效的選擇。\")\n",
    "            return\n",
    "\n",
    "        # 繪製曲線\n",
    "        plot_from_csv(selected_csv_path)\n",
    "\n",
    "    else:\n",
    "        print(f\"不支援的操作: {operation}\")\n",
    "        sys.exit(1)   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
